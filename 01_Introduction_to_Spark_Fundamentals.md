# Introduction to Spark Fundamentals

## Purpose of Spark:
- Spark is designed to be fast, general-purpose, and easy to use.
- It is ideal for processing large-scale data quickly, replacing the slower MapReduce model.

## Why Use Spark:
- It allows in-memory computations, which are faster than disk-based operations in MapReduce.
- It supports various workloads, including batch processing, interactive queries, and streaming data.

## Components of the Spark Unified Stack:
- Spark provides additional libraries for SQL, machine learning, streaming, and graph processing.
- It can run on Hadoop (YARN), Apache Mesos, or as a standalone.

## Ease of Use:
- Spark provides simple APIs for Scala, Python, and Java.
- The platform is mature and reliable with fault tolerance and scalability.

## Two Key User Groups:
- **Data Scientists**: Use Spark for ad-hoc analysis, running interactive queries, and obtaining insights.
- **Engineers**: Use Spark to build production systems, parallelize applications across clusters, and optimize performance.

## Key Features Over MapReduce:
- In-memory distributed computing.
- Low latency.
- High-level APIs and tools for a wide range of applications.
